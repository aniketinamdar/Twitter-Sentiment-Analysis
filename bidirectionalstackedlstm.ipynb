{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aniketinamdar/Twitter-Sentiment-Analysis-using-Support-Vector-Machine-/blob/main/bidirectionalstackedlstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6MVMV9EP08fw",
        "outputId": "c532da72-81d3-4eab-bb05-4b11876eab1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-03-17 06:15:14.584041: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-03-17 06:15:15.443408: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-03-17 06:15:15.443536: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-03-17 06:15:15.443554: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-sm==3.4.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.1/en_core_web_sm-3.4.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.9/dist-packages (from en-core-web-sm==3.4.1) (3.4.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.9/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (6.3.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.9/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.10.6)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.25.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (23.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.9)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.65.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.1.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.3.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.9/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.8)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.9/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.4)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.9/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.4.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (63.4.3)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.9/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.9/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.22.4)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.7)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.9/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.26.15)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.10)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.9/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDrH57g9NCb5",
        "outputId": "b5c5f3a4-1510-400a-9fd4-2eae64963f07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('all')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNj4QD0i6nw5",
        "outputId": "141b16e7-5ffa-45c9-d37d-3330e5df7a1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2022 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet2022.zip.\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import spacy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import en_core_web_sm\n",
        "import tensorflow as tf\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Sequential\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "26YHzdRD3rKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/Dataset/Twitter_Data.csv',nrows=10000)"
      ],
      "metadata": {
        "id": "CvTe5TL04uCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['category'].fillna(0, inplace=True)"
      ],
      "metadata": {
        "id": "qzMIXg0Y-Noc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.clean_text = df.clean_text.fillna('')"
      ],
      "metadata": {
        "id": "RSEacrNdA3d8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = en_core_web_sm.load()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stopwords = STOP_WORDS\n",
        "EMOJI_PATTERN = re.compile(\n",
        "    \"[\"\n",
        "    u\"U0001F600-U0001F64F\"  # emoticons\n",
        "    u\"U0001F300-U0001F5FF\"  # symbols & pictographs\n",
        "    u\"U0001F680-U0001F6FF\"  # transport & map symbols\n",
        "    u\"U0001F1E0-U0001F1FF\"  # flags (iOS)\n",
        "    u\"U00002702-U000027B0\"\n",
        "    u\"U000024C2-U0001F251\"\n",
        "    \"]+\", flags=re.UNICODE\n",
        "  )\n",
        "FILTERS = '!\"#$%&()*+,-/:;?@[\\]^_`{|}~tn'\n",
        "HTML_TAG_PATTERN = re.compile(r']*>')\n",
        "NUMBERING_PATTERN = re.compile('d+(?:st|[nr]d|th)')\n",
        "DISABLE_PIPELINES = [\"tok2vec\", \"parser\", \"ner\", \"textcat\", \"custom\", \"lemmatizer\"]"
      ],
      "metadata": {
        "id": "zvr11Vvw3r-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initial_preprocessing(text):\n",
        "    tag_removed_text = HTML_TAG_PATTERN.sub('', text)\n",
        "    emoji_removed_text = EMOJI_PATTERN.sub(r'', tag_removed_text)\n",
        "    numberings_removed_text =  NUMBERING_PATTERN.sub('', emoji_removed_text)\n",
        "    extra_chars_removed_text = re.sub(\n",
        "        r\"(.)1{2,}\",  r'11', numberings_removed_text\n",
        "    )\n",
        "    return extra_chars_removed_text"
      ],
      "metadata": {
        "id": "H8fbzLkY368Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(doc):\n",
        "    tokens = [\n",
        "        token\n",
        "        for token in doc\n",
        "        if not token.is_space and\n",
        "           not token.like_email and\n",
        "           not token.like_url and\n",
        "           not token.is_stop and\n",
        "           not token.is_punct and\n",
        "           not token.like_num\n",
        "    ]\n",
        "    translation_table = str.maketrans('', '', FILTERS)\n",
        "    translated_tokens = [\n",
        "        token.text.lower().translate(translation_table)\n",
        "        for token in tokens\n",
        "    ]\n",
        "    lemmatized_tokens = [\n",
        "        lemmatizer.lemmatize(token)\n",
        "        for token in translated_tokens\n",
        "        if len(token) > 1\n",
        "    ]\n",
        "    return lemmatized_tokens"
      ],
      "metadata": {
        "id": "RaX9txSM5Jig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MsdWJE_YPfz-",
        "outputId": "8be9138e-7bb0-449c-e755-81c5b460d5be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels = df['category'].iloc[:10000]\n",
        "data = df.iloc[:10000, :]\n",
        "column = 'clean_text'\n",
        "not_null_data = data[data[column].notnull()]\n",
        "not_null_data[column] = not_null_data[column].apply(initial_preprocessing)\n",
        "texts = [\n",
        "    preprocess_text(doc)\n",
        "    for doc in nlp.pipe(not_null_data[column], disable=DISABLE_PIPELINES)\n",
        "]"
      ],
      "metadata": {
        "id": "87nnBeDQ6PqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(\n",
        "    filters=FILTERS,\n",
        "    lower=True\n",
        ")\n",
        "padding = 'post'\n",
        "tokenizer.fit_on_texts(texts)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "sequences = []\n",
        "max_sequence_len = 0\n",
        "for text in texts:\n",
        "    txt_to_seq = tokenizer.texts_to_sequences([text])[0]\n",
        "    sequences.append(txt_to_seq)\n",
        "    txt_to_seq_len = len(txt_to_seq)\n",
        "    if txt_to_seq_len > max_sequence_len:\n",
        "        max_sequence_len = txt_to_seq_len\n",
        "padded_sequences = pad_sequences(\n",
        "    sequences, \n",
        "    maxlen=max_sequence_len, \n",
        "    padding=padding\n",
        ")"
      ],
      "metadata": {
        "id": "cU6nN2Qb4Ayq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 64, input_length=max_sequence_len)) # update input length to match actual sequence length\n",
        "model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Bidirectional(LSTM(32)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "adam = Adam(learning_rate=0.01)\n",
        "model.compile(\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), \n",
        "    optimizer=adam, \n",
        "    metrics=['accuracy']\n",
        ")\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVmRXtBa4Els",
        "outputId": "2c475403-4560-41fe-fdc7-bee65392086e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_5 (Embedding)     (None, 39, 64)            1088000   \n",
            "                                                                 \n",
            " bidirectional_10 (Bidirecti  (None, 39, 128)          66048     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dropout_15 (Dropout)        (None, 39, 128)           0         \n",
            "                                                                 \n",
            " bidirectional_11 (Bidirecti  (None, 64)               41216     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dropout_16 (Dropout)        (None, 64)                0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 64)                4160      \n",
            "                                                                 \n",
            " dropout_17 (Dropout)        (None, 64)                0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,199,489\n",
            "Trainable params: 1,199,489\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "padded_sequences.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhY0xA7y7clP",
        "outputId": "bce5bdfc-09d3-48bc-d044-9cffbf95c92b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 39)"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xj6Z-fp17f9X",
        "outputId": "d5028d7c-82d1-4e4e-995a-f66d92e8e487"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000,)"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KwkyO2IH_NNu",
        "outputId": "408dd5b6-9f69-4254-c603-ce0720557dc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 10000 entries, 0 to 9999\n",
            "Data columns (total 2 columns):\n",
            " #   Column      Non-Null Count  Dtype \n",
            "---  ------      --------------  ----- \n",
            " 0   clean_text  10000 non-null  object\n",
            " 1   category    10000 non-null  int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 156.4+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    padded_sequences, \n",
        "    labels, \n",
        "    epochs=100,\n",
        "    verbose=1,\n",
        "    batch_size=64)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3pUp4EfI4KT2",
        "outputId": "ceeedbdb-6d92-49c2-a4cd-4ac561cab39e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5676: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "157/157 [==============================] - 35s 127ms/step - loss: 0.0184 - accuracy: 0.3781\n",
            "Epoch 2/100\n",
            "157/157 [==============================] - 8s 53ms/step - loss: -726.7120 - accuracy: 0.4454\n",
            "Epoch 3/100\n",
            "157/157 [==============================] - 5s 29ms/step - loss: -5943.0127 - accuracy: 0.5205\n",
            "Epoch 4/100\n",
            "157/157 [==============================] - 4s 28ms/step - loss: -18971.3926 - accuracy: 0.5261\n",
            "Epoch 5/100\n",
            "157/157 [==============================] - 4s 29ms/step - loss: -41043.0312 - accuracy: 0.5571\n",
            "Epoch 6/100\n",
            "157/157 [==============================] - 4s 21ms/step - loss: -70093.4375 - accuracy: 0.5593\n",
            "Epoch 7/100\n",
            "157/157 [==============================] - 3s 17ms/step - loss: -104904.8281 - accuracy: 0.5790\n",
            "Epoch 8/100\n",
            "157/157 [==============================] - 3s 22ms/step - loss: -145659.3594 - accuracy: 0.5728\n",
            "Epoch 9/100\n",
            "157/157 [==============================] - 3s 17ms/step - loss: -192034.1250 - accuracy: 0.5759\n",
            "Epoch 10/100\n",
            "157/157 [==============================] - 3s 22ms/step - loss: -246451.4531 - accuracy: 0.5852\n",
            "Epoch 11/100\n",
            "157/157 [==============================] - 2s 14ms/step - loss: -300576.0000 - accuracy: 0.5788\n",
            "Epoch 12/100\n",
            "157/157 [==============================] - 2s 14ms/step - loss: -365413.5000 - accuracy: 0.5661\n",
            "Epoch 13/100\n",
            "157/157 [==============================] - 2s 15ms/step - loss: -436416.0000 - accuracy: 0.5487\n",
            "Epoch 14/100\n",
            "157/157 [==============================] - 2s 13ms/step - loss: -515743.0312 - accuracy: 0.5675\n",
            "Epoch 15/100\n",
            "157/157 [==============================] - 3s 18ms/step - loss: -591517.8125 - accuracy: 0.5603\n",
            "Epoch 16/100\n",
            "157/157 [==============================] - 3s 16ms/step - loss: -669513.1875 - accuracy: 0.5661\n",
            "Epoch 17/100\n",
            "157/157 [==============================] - 2s 15ms/step - loss: -749387.6250 - accuracy: 0.5603\n",
            "Epoch 18/100\n",
            "157/157 [==============================] - 2s 15ms/step - loss: -846253.7500 - accuracy: 0.5479\n",
            "Epoch 19/100\n",
            "157/157 [==============================] - 2s 14ms/step - loss: -942218.4375 - accuracy: 0.5599\n",
            "Epoch 20/100\n",
            "157/157 [==============================] - 2s 13ms/step - loss: -1019220.5000 - accuracy: 0.5593\n",
            "Epoch 21/100\n",
            "157/157 [==============================] - 3s 20ms/step - loss: -1139296.1250 - accuracy: 0.5611\n",
            "Epoch 22/100\n",
            "157/157 [==============================] - 2s 15ms/step - loss: -1250673.8750 - accuracy: 0.5635\n",
            "Epoch 23/100\n",
            "157/157 [==============================] - 2s 15ms/step - loss: -1369334.5000 - accuracy: 0.5692\n",
            "Epoch 24/100\n",
            "157/157 [==============================] - 2s 14ms/step - loss: -1471087.3750 - accuracy: 0.5657\n",
            "Epoch 25/100\n",
            "157/157 [==============================] - 2s 14ms/step - loss: -1595079.3750 - accuracy: 0.5716\n",
            "Epoch 26/100\n",
            "157/157 [==============================] - 3s 18ms/step - loss: -1733715.3750 - accuracy: 0.5775\n",
            "Epoch 27/100\n",
            "157/157 [==============================] - 3s 19ms/step - loss: -1819111.2500 - accuracy: 0.5771\n",
            "Epoch 28/100\n",
            "157/157 [==============================] - 2s 14ms/step - loss: -1923504.1250 - accuracy: 0.5769\n",
            "Epoch 29/100\n",
            "157/157 [==============================] - 2s 12ms/step - loss: -2062914.5000 - accuracy: 0.5793\n",
            "Epoch 30/100\n",
            "157/157 [==============================] - 2s 11ms/step - loss: -2189547.7500 - accuracy: 0.5774\n",
            "Epoch 31/100\n",
            "157/157 [==============================] - 2s 13ms/step - loss: -2326003.0000 - accuracy: 0.5810\n",
            "Epoch 32/100\n",
            "157/157 [==============================] - 2s 11ms/step - loss: -2453779.0000 - accuracy: 0.5644\n",
            "Epoch 33/100\n",
            "157/157 [==============================] - 3s 18ms/step - loss: -2608627.5000 - accuracy: 0.5706\n",
            "Epoch 34/100\n",
            "157/157 [==============================] - 3s 18ms/step - loss: -2722780.2500 - accuracy: 0.5714\n",
            "Epoch 35/100\n",
            "157/157 [==============================] - 2s 13ms/step - loss: -2912544.2500 - accuracy: 0.5719\n",
            "Epoch 36/100\n",
            "157/157 [==============================] - 2s 12ms/step - loss: -3077285.7500 - accuracy: 0.5730\n",
            "Epoch 37/100\n",
            "157/157 [==============================] - 2s 13ms/step - loss: -3191149.5000 - accuracy: 0.5831\n",
            "Epoch 38/100\n",
            "157/157 [==============================] - 2s 13ms/step - loss: -3366378.0000 - accuracy: 0.5894\n",
            "Epoch 39/100\n",
            "157/157 [==============================] - 2s 15ms/step - loss: -3516009.7500 - accuracy: 0.5895\n",
            "Epoch 40/100\n",
            "157/157 [==============================] - 2s 15ms/step - loss: -3676920.2500 - accuracy: 0.5868\n",
            "Epoch 41/100\n",
            "157/157 [==============================] - 2s 13ms/step - loss: -3854327.7500 - accuracy: 0.5858\n",
            "Epoch 42/100\n",
            "157/157 [==============================] - 2s 12ms/step - loss: -4070731.7500 - accuracy: 0.5935\n",
            "Epoch 43/100\n",
            "157/157 [==============================] - 2s 12ms/step - loss: -4268529.0000 - accuracy: 0.5827\n",
            "Epoch 44/100\n",
            "157/157 [==============================] - 2s 12ms/step - loss: -4438969.5000 - accuracy: 0.5691\n",
            "Epoch 45/100\n",
            "157/157 [==============================] - 2s 12ms/step - loss: -4609107.5000 - accuracy: 0.5721\n",
            "Epoch 46/100\n",
            "157/157 [==============================] - 3s 19ms/step - loss: -4765407.0000 - accuracy: 0.5671\n",
            "Epoch 47/100\n",
            "157/157 [==============================] - 2s 12ms/step - loss: -4973590.5000 - accuracy: 0.5882\n",
            "Epoch 48/100\n",
            "157/157 [==============================] - 2s 13ms/step - loss: -5185150.0000 - accuracy: 0.5823\n",
            "Epoch 49/100\n",
            "157/157 [==============================] - 2s 13ms/step - loss: -5354927.5000 - accuracy: 0.5740\n",
            "Epoch 50/100\n",
            "157/157 [==============================] - 2s 13ms/step - loss: -5553683.5000 - accuracy: 0.5656\n",
            "Epoch 51/100\n",
            "157/157 [==============================] - 2s 13ms/step - loss: -5740852.5000 - accuracy: 0.5702\n",
            "Epoch 52/100\n",
            "157/157 [==============================] - 3s 19ms/step - loss: -5801077.5000 - accuracy: 0.5671\n",
            "Epoch 53/100\n",
            "157/157 [==============================] - 2s 15ms/step - loss: -6006998.0000 - accuracy: 0.5697\n",
            "Epoch 54/100\n",
            "157/157 [==============================] - 2s 12ms/step - loss: -6264325.5000 - accuracy: 0.5699\n",
            "Epoch 55/100\n",
            "157/157 [==============================] - 2s 14ms/step - loss: -6556369.5000 - accuracy: 0.5612\n",
            "Epoch 56/100\n",
            "157/157 [==============================] - 2s 14ms/step - loss: -6785854.5000 - accuracy: 0.5615\n",
            "Epoch 57/100\n",
            "157/157 [==============================] - 2s 13ms/step - loss: -6720737.5000 - accuracy: 0.5591\n",
            "Epoch 58/100\n",
            "157/157 [==============================] - 3s 20ms/step - loss: -7116135.0000 - accuracy: 0.5592\n",
            "Epoch 59/100\n",
            "157/157 [==============================] - 2s 14ms/step - loss: -7349910.5000 - accuracy: 0.5571\n",
            "Epoch 60/100\n",
            "157/157 [==============================] - 2s 12ms/step - loss: -7617951.5000 - accuracy: 0.5563\n",
            "Epoch 61/100\n",
            "157/157 [==============================] - 2s 11ms/step - loss: -7814043.0000 - accuracy: 0.5517\n",
            "Epoch 62/100\n",
            "157/157 [==============================] - 2s 14ms/step - loss: -8056963.0000 - accuracy: 0.5473\n",
            "Epoch 63/100\n",
            "157/157 [==============================] - 2s 12ms/step - loss: -8270864.5000 - accuracy: 0.5398\n",
            "Epoch 64/100\n",
            "157/157 [==============================] - 2s 15ms/step - loss: -8601651.0000 - accuracy: 0.5478\n",
            "Epoch 65/100\n",
            "157/157 [==============================] - 3s 16ms/step - loss: -8706242.0000 - accuracy: 0.5459\n",
            "Epoch 66/100\n",
            "157/157 [==============================] - 2s 13ms/step - loss: -8983813.0000 - accuracy: 0.5376\n",
            "Epoch 67/100\n",
            "157/157 [==============================] - 2s 12ms/step - loss: -9260982.0000 - accuracy: 0.5357\n",
            "Epoch 68/100\n",
            "157/157 [==============================] - 2s 12ms/step - loss: -9457160.0000 - accuracy: 0.5347\n",
            "Epoch 69/100\n",
            "157/157 [==============================] - 2s 12ms/step - loss: -9628674.0000 - accuracy: 0.5453\n",
            "Epoch 70/100\n",
            "157/157 [==============================] - 2s 13ms/step - loss: -9801040.0000 - accuracy: 0.5485\n",
            "Epoch 71/100\n",
            "157/157 [==============================] - 2s 16ms/step - loss: -10106060.0000 - accuracy: 0.5326\n",
            "Epoch 72/100\n",
            "157/157 [==============================] - 2s 14ms/step - loss: -10444491.0000 - accuracy: 0.5461\n",
            "Epoch 73/100\n",
            "157/157 [==============================] - 2s 11ms/step - loss: -10786305.0000 - accuracy: 0.5398\n",
            "Epoch 74/100\n",
            "157/157 [==============================] - 2s 11ms/step - loss: -10964387.0000 - accuracy: 0.5407\n",
            "Epoch 75/100\n",
            "157/157 [==============================] - 2s 12ms/step - loss: -11280342.0000 - accuracy: 0.5492\n",
            "Epoch 76/100\n",
            "157/157 [==============================] - 2s 11ms/step - loss: -11477697.0000 - accuracy: 0.5423\n",
            "Epoch 77/100\n",
            "157/157 [==============================] - 3s 17ms/step - loss: -11529862.0000 - accuracy: 0.5510\n",
            "Epoch 78/100\n",
            "157/157 [==============================] - 2s 16ms/step - loss: -11502985.0000 - accuracy: 0.5476\n",
            "Epoch 79/100\n",
            "157/157 [==============================] - 2s 11ms/step - loss: -11629766.0000 - accuracy: 0.5456\n",
            "Epoch 80/100\n",
            "157/157 [==============================] - 2s 13ms/step - loss: -12329080.0000 - accuracy: 0.5435\n",
            "Epoch 81/100\n",
            "157/157 [==============================] - 2s 13ms/step - loss: -12727017.0000 - accuracy: 0.5394\n",
            "Epoch 82/100\n",
            "157/157 [==============================] - 2s 12ms/step - loss: -13220538.0000 - accuracy: 0.5430\n",
            "Epoch 83/100\n",
            "157/157 [==============================] - 2s 12ms/step - loss: -13612520.0000 - accuracy: 0.5421\n",
            "Epoch 84/100\n",
            "157/157 [==============================] - 3s 18ms/step - loss: -14021986.0000 - accuracy: 0.5433\n",
            "Epoch 85/100\n",
            "157/157 [==============================] - 2s 14ms/step - loss: -14274142.0000 - accuracy: 0.5337\n",
            "Epoch 86/100\n",
            "157/157 [==============================] - 2s 11ms/step - loss: -14551769.0000 - accuracy: 0.5335\n",
            "Epoch 87/100\n",
            "157/157 [==============================] - 2s 12ms/step - loss: -14909297.0000 - accuracy: 0.5403\n",
            "Epoch 88/100\n",
            "157/157 [==============================] - 2s 12ms/step - loss: -15241401.0000 - accuracy: 0.5428\n",
            "Epoch 89/100\n",
            "157/157 [==============================] - 2s 11ms/step - loss: -15528908.0000 - accuracy: 0.5358\n",
            "Epoch 90/100\n",
            "157/157 [==============================] - 2s 15ms/step - loss: -15520739.0000 - accuracy: 0.5205\n",
            "Epoch 91/100\n",
            "157/157 [==============================] - 3s 16ms/step - loss: -15932442.0000 - accuracy: 0.5265\n",
            "Epoch 92/100\n",
            "157/157 [==============================] - 2s 14ms/step - loss: -16200424.0000 - accuracy: 0.5301\n",
            "Epoch 93/100\n",
            "157/157 [==============================] - 2s 12ms/step - loss: -16680383.0000 - accuracy: 0.5385\n",
            "Epoch 94/100\n",
            "157/157 [==============================] - 2s 12ms/step - loss: -17035616.0000 - accuracy: 0.5404\n",
            "Epoch 95/100\n",
            "157/157 [==============================] - 2s 12ms/step - loss: -17200524.0000 - accuracy: 0.5446\n",
            "Epoch 96/100\n",
            "157/157 [==============================] - 2s 12ms/step - loss: -17580994.0000 - accuracy: 0.5422\n",
            "Epoch 97/100\n",
            "157/157 [==============================] - 2s 15ms/step - loss: -18036230.0000 - accuracy: 0.5420\n",
            "Epoch 98/100\n",
            "157/157 [==============================] - 2s 15ms/step - loss: -18081252.0000 - accuracy: 0.5474\n",
            "Epoch 99/100\n",
            "157/157 [==============================] - 2s 12ms/step - loss: -18421140.0000 - accuracy: 0.5531\n",
            "Epoch 100/100\n",
            "157/157 [==============================] - 2s 12ms/step - loss: -19004328.0000 - accuracy: 0.5530\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict(padded_sequences[:4])\n",
        "for pred in predictions:\n",
        "    print(pred[0])"
      ],
      "metadata": {
        "id": "p2VpQo018tPT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "609b5064-6612-434d-fe79-b70b14e30f32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 1s/step\n",
            "0.0\n",
            "1.2726635e-07\n",
            "1.0\n",
            "1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get predictions for custom input\n",
        "custom_input = input(\"Enter a line:\" )\n",
        "processed_input = preprocess_text(nlp(custom_input))\n",
        "custom_seq = tokenizer.texts_to_sequences([processed_input])[0]\n",
        "custom_padded_seq = pad_sequences([custom_seq], maxlen=max_sequence_len, padding=padding)\n",
        "custom_pred = model.predict(custom_padded_seq)\n",
        "\n",
        "# Calculate median of all predictions\n",
        "all_preds = model.predict(padded_sequences)\n",
        "median_pred = np.median(all_preds)\n",
        "print(median_pred)\n",
        "# Compare custom prediction with median value\n",
        "if custom_pred > median_pred:\n",
        "    print(\"Positive sentiment\")\n",
        "else:\n",
        "    print(\"Negative sentiment\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9yVxRun8Ucaa",
        "outputId": "94077fef-0e5d-45f9-c501-89816ecb0582"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a line:have a good day\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "313/313 [==============================] - 2s 6ms/step\n",
            "5.3622813e-09\n",
            "Positive sentiment\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into training and testing sets\n",
        "train_data, test_data, train_labels, test_labels = train_test_split(\n",
        "    padded_sequences, labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train model on training set\n",
        "history = model.fit(\n",
        "    train_data,\n",
        "    train_labels,\n",
        "    epochs=50,\n",
        "    verbose=1,\n",
        "    batch_size=64\n",
        ")\n",
        "\n",
        "# Make predictions on testing set\n",
        "test_predictions = model.predict(test_data)\n",
        "\n",
        "# Convert predictions to binary labels\n",
        "test_predictions = np.where(test_predictions > np.median(predictions), 1, 0)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ytOlz_y3WxSw",
        "outputId": "21816981-e967-4469-9773-25450c8c8e6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "125/125 [==============================] - 2s 13ms/step - loss: -58386528.0000 - accuracy: 0.5390\n",
            "Epoch 2/50\n",
            "125/125 [==============================] - 2s 14ms/step - loss: -59500068.0000 - accuracy: 0.5385\n",
            "Epoch 3/50\n",
            "125/125 [==============================] - 2s 14ms/step - loss: -60896556.0000 - accuracy: 0.5409\n",
            "Epoch 4/50\n",
            "125/125 [==============================] - 2s 16ms/step - loss: -60506620.0000 - accuracy: 0.5332\n",
            "Epoch 5/50\n",
            "125/125 [==============================] - 2s 13ms/step - loss: -60918264.0000 - accuracy: 0.5420\n",
            "Epoch 6/50\n",
            "125/125 [==============================] - 1s 11ms/step - loss: -62138132.0000 - accuracy: 0.5362\n",
            "Epoch 7/50\n",
            "125/125 [==============================] - 1s 12ms/step - loss: -62731996.0000 - accuracy: 0.5282\n",
            "Epoch 8/50\n",
            "125/125 [==============================] - 1s 12ms/step - loss: -63733792.0000 - accuracy: 0.5290\n",
            "Epoch 9/50\n",
            "125/125 [==============================] - 1s 12ms/step - loss: -63820172.0000 - accuracy: 0.5272\n",
            "Epoch 10/50\n",
            "125/125 [==============================] - 1s 12ms/step - loss: -64195896.0000 - accuracy: 0.5220\n",
            "Epoch 11/50\n",
            "125/125 [==============================] - 1s 12ms/step - loss: -65130008.0000 - accuracy: 0.5224\n",
            "Epoch 12/50\n",
            "125/125 [==============================] - 2s 16ms/step - loss: -65204132.0000 - accuracy: 0.5265\n",
            "Epoch 13/50\n",
            "125/125 [==============================] - 2s 16ms/step - loss: -66029208.0000 - accuracy: 0.5295\n",
            "Epoch 14/50\n",
            "125/125 [==============================] - 1s 12ms/step - loss: -66354744.0000 - accuracy: 0.5236\n",
            "Epoch 15/50\n",
            "125/125 [==============================] - 1s 11ms/step - loss: -66943036.0000 - accuracy: 0.5221\n",
            "Epoch 16/50\n",
            "125/125 [==============================] - 1s 12ms/step - loss: -67653912.0000 - accuracy: 0.5205\n",
            "Epoch 17/50\n",
            "125/125 [==============================] - 1s 12ms/step - loss: -66634048.0000 - accuracy: 0.5170\n",
            "Epoch 18/50\n",
            "125/125 [==============================] - 1s 12ms/step - loss: -68157376.0000 - accuracy: 0.5265\n",
            "Epoch 19/50\n",
            "125/125 [==============================] - 1s 12ms/step - loss: -68859976.0000 - accuracy: 0.5281\n",
            "Epoch 20/50\n",
            "125/125 [==============================] - 2s 14ms/step - loss: -68912200.0000 - accuracy: 0.5164\n",
            "Epoch 21/50\n",
            "125/125 [==============================] - 2s 19ms/step - loss: -69358040.0000 - accuracy: 0.5224\n",
            "Epoch 22/50\n",
            "125/125 [==============================] - 2s 14ms/step - loss: -70163864.0000 - accuracy: 0.5167\n",
            "Epoch 23/50\n",
            "125/125 [==============================] - 2s 12ms/step - loss: -70751488.0000 - accuracy: 0.5297\n",
            "Epoch 24/50\n",
            "125/125 [==============================] - 1s 11ms/step - loss: -70796632.0000 - accuracy: 0.5155\n",
            "Epoch 25/50\n",
            "125/125 [==============================] - 1s 12ms/step - loss: -71634256.0000 - accuracy: 0.5199\n",
            "Epoch 26/50\n",
            "125/125 [==============================] - 1s 11ms/step - loss: -72576160.0000 - accuracy: 0.5171\n",
            "Epoch 27/50\n",
            "125/125 [==============================] - 1s 11ms/step - loss: -72683456.0000 - accuracy: 0.5207\n",
            "Epoch 28/50\n",
            "125/125 [==============================] - 1s 11ms/step - loss: -73145112.0000 - accuracy: 0.5255\n",
            "Epoch 29/50\n",
            "125/125 [==============================] - 2s 17ms/step - loss: -73612320.0000 - accuracy: 0.5240\n",
            "Epoch 30/50\n",
            "125/125 [==============================] - 2s 16ms/step - loss: -73216304.0000 - accuracy: 0.5197\n",
            "Epoch 31/50\n",
            "125/125 [==============================] - 1s 12ms/step - loss: -74728232.0000 - accuracy: 0.5238\n",
            "Epoch 32/50\n",
            "125/125 [==============================] - 1s 11ms/step - loss: -74669680.0000 - accuracy: 0.5180\n",
            "Epoch 33/50\n",
            "125/125 [==============================] - 2s 13ms/step - loss: -76042376.0000 - accuracy: 0.5241\n",
            "Epoch 34/50\n",
            "125/125 [==============================] - 1s 12ms/step - loss: -76490048.0000 - accuracy: 0.5244\n",
            "Epoch 35/50\n",
            "125/125 [==============================] - 1s 12ms/step - loss: -76955912.0000 - accuracy: 0.5226\n",
            "Epoch 36/50\n",
            "125/125 [==============================] - 1s 12ms/step - loss: -76458224.0000 - accuracy: 0.5201\n",
            "Epoch 37/50\n",
            "125/125 [==============================] - 2s 14ms/step - loss: -77441984.0000 - accuracy: 0.5190\n",
            "Epoch 38/50\n",
            "125/125 [==============================] - 2s 17ms/step - loss: -77953520.0000 - accuracy: 0.5169\n",
            "Epoch 39/50\n",
            "125/125 [==============================] - 2s 13ms/step - loss: -79287528.0000 - accuracy: 0.5080\n",
            "Epoch 40/50\n",
            "125/125 [==============================] - 2s 12ms/step - loss: -80076312.0000 - accuracy: 0.5155\n",
            "Epoch 41/50\n",
            "125/125 [==============================] - 1s 12ms/step - loss: -80119144.0000 - accuracy: 0.5194\n",
            "Epoch 42/50\n",
            "125/125 [==============================] - 2s 14ms/step - loss: -81571944.0000 - accuracy: 0.5201\n",
            "Epoch 43/50\n",
            "125/125 [==============================] - 2s 12ms/step - loss: -81387056.0000 - accuracy: 0.5250\n",
            "Epoch 44/50\n",
            "125/125 [==============================] - 1s 12ms/step - loss: -83206232.0000 - accuracy: 0.5263\n",
            "Epoch 45/50\n",
            "125/125 [==============================] - 2s 13ms/step - loss: -83537504.0000 - accuracy: 0.5309\n",
            "Epoch 46/50\n",
            "125/125 [==============================] - 2s 17ms/step - loss: -84278872.0000 - accuracy: 0.5295\n",
            "Epoch 47/50\n",
            "125/125 [==============================] - 2s 13ms/step - loss: -85242864.0000 - accuracy: 0.5268\n",
            "Epoch 48/50\n",
            "125/125 [==============================] - 1s 12ms/step - loss: -85748960.0000 - accuracy: 0.5288\n",
            "Epoch 49/50\n",
            "125/125 [==============================] - 1s 12ms/step - loss: -86097072.0000 - accuracy: 0.5228\n",
            "Epoch 50/50\n",
            "125/125 [==============================] - 2s 12ms/step - loss: -87090912.0000 - accuracy: 0.5254\n",
            "63/63 [==============================] - 0s 5ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(test_labels, test_predictions)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmzWtwTCZhmt",
        "outputId": "549ddbd3-9fb8-4159-e501-63c55ac2b3b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.4965\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zYuAKW0Oa7PB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}